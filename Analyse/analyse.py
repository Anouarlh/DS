# -*- coding: utf-8 -*-
"""analyse.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14owaC2SBke-uLNY9CCZbIvwC7LS2gaY-
"""

import pandas as pd

# Please ensure the file 'AI_Impact_on_Jobs_2030.csv' is in the correct directory.
# For example, if it's in the root of your Colab environment, the path would be '/AI_Impact_on_Jobs_2030.csv'
# If it's in a subdirectory like 'content/sample_data/', adjust the path accordingly.
try:
    df = pd.read_csv("/AI_Impact_on_Jobs_2030.csv")
    print("CSV file loaded successfully into a DataFrame named 'df'.")
except FileNotFoundError:
    print("Error: The file '/AI_Impact_on_Jobs_2030.csv' was not found.")
    print("Please check the file path or upload the file to your Colab environment.")
except Exception as e:
    print(f"An error occurred while reading the CSV file: {e}")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Charger les données
df = pd.read_csv('/AI_Impact_on_Jobs_2030.csv')

# Encodage des variables catégorielles
label_encoders = {}
categorical_cols = ['Job_Title', 'Education_Level', 'Risk_Category']

for col in categorical_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le  # pour inverser si besoin

# Afficher les mappings
print("Encodage Job_Title :", dict(zip(label_encoders['Job_Title'].classes_,
                                       label_encoders['Job_Title'].transform(label_encoders['Job_Title'].classes_))))
print("Encodage Risk_Category :", dict(zip(label_encoders['Risk_Category'].classes_,
                                           label_encoders['Risk_Category'].transform(label_encoders['Risk_Category'].classes_))))

# Vérification
df.isnull().sum()

# Stratégie selon le % de données manquantes
# <5% : suppression
# 5-20% : imputation (moyenne/médiane/mode)
# >20% : création variable indicateur

!pip install category_encoders

# 3 stratégies possibles :

# 1. Label Encoding (pour arbres)
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['Job_Title_encoded'] = le.fit_transform(df['Job_Title'])

# 2. One-Hot Encoding (pour régression)
df_encoded = pd.get_dummies(df, columns=['Job_Title', 'Education_Level'])

# 3. Target Encoding (plus performant)
from category_encoders import TargetEncoder
encoder = TargetEncoder()
df['Job_Title_te'] = encoder.fit_transform(df['Job_Title'], df['Automation_Probability_2030'])

# Création de nouvelles variables
df['Salary_Experience_Ratio'] = df['Average_Salary'] / (df['Years_Experience'] + 1)

# Correction: Sélectionner les colonnes Skill_1 à Skill_10
skill_cols = [f'Skill_{i}' for i in range(1, 11)]
df['Total_Skills_Score'] = df[skill_cols].sum(axis=1)

df['AI_Tech_Interaction'] = df['AI_Exposure_Index'] * df['Tech_Growth_Factor']

# Binning des variables continues
df['Experience_Level'] = pd.cut(df['Years_Experience'],
                                 bins=[0, 5, 15, 30],
                                 labels=['Junior', 'Mid', 'Senior'])

from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Définir X avec les colonnes numériques pertinentes à scaler
# Ajustez cette sélection de colonnes en fonction de votre modèle
X = df[['Average_Salary', 'Years_Experience', 'AI_Exposure_Index', 'Tech_Growth_Factor',
        'Automation_Probability_2030', 'Total_Skills_Score', 'AI_Tech_Interaction']]

# StandardScaler (pour régression, SVM)
scaler = StandardScaler()
X_scaled_standard = scaler.fit_transform(X)
print("X_scaled_standard shape:", X_scaled_standard.shape)

# MinMaxScaler (pour réseaux de neurones)
scaler = MinMaxScaler()
X_scaled_minmax = scaler.fit_transform(X)
print("X_scaled_minmax shape:", X_scaled_minmax.shape)

import matplotlib.pyplot as plt
import seaborn as sns

# Combiner X et y pour la matrice de corrélation si y est la cible
# S'assurer que 'Automation_Probability_2030' est présente dans le DataFrame pour la corrélation
# Pour inclure la cible dans la matrice de corrélation, nous allons créer un df temporaire

# Sélectionnez les colonnes numériques à inclure dans la heatmap, y compris la cible
# Nous allons utiliser le 'numerical_df' créé précédemment qui inclut la cible
# Ou bien, recréer un df avec X et y pour s'assurer d'avoir toutes les colonnes pertinentes.

# Assurez-vous que X et y sont des DataFrames/Series Pandas pour la concaténation facile
if isinstance(X, pd.DataFrame):
    df_for_corr = X.copy()
else:
    df_for_corr = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])]) # Gérer le cas où X est un numpy array

if isinstance(y, pd.Series):
    df_for_corr['Automation_Probability_2030'] = y
else:
    # Si y n'est pas une Series, il faut s'assurer qu'il s'agit d'un tableau numpy compatible
    df_for_corr['Automation_Probability_2030'] = pd.Series(y, index=df_for_corr.index)


# Calculer la matrice de corrélation
correlation_matrix = df_for_corr.corr()

plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Matrice de Corrélation des Variables Numériques', fontsize=16)
plt.show()

# 1. Corrélation avec la cible
# Sélectionner uniquement les colonnes numériques pour le calcul de corrélation
numerical_df = df.select_dtypes(include=['number'])
correlation_with_target = numerical_df.corr()['Automation_Probability_2030'].abs().sort_values(ascending=False)

# 2. Variance Inflation Factor (VIF) pour multicollinéarité
from statsmodels.stats.outliers_influence import variance_inflation_factor
vif_data = pd.DataFrame()
vif_data['feature'] = X.columns
vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]
# Supprimer VIF > 10

# 3. Sélection récursive
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestRegressor
selector = RFE(RandomForestRegressor(), n_features_to_select=10)
# X_train et y_train ne sont pas définis ici, cette ligne lèvera une erreur si elle est exécutée telle quelle.
# Il faudrait d'abord diviser les données en ensembles d'entraînement et de test.
# selector.fit(X_train, y_train)
# selected_features = X.columns[selector.support_]

from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
import xgboost as xgb
import lightgbm as lgb

models = {
    'Régression Linéaire': LinearRegression(),
    'Ridge Regression': Ridge(alpha=1.0),
    'Lasso Regression': Lasso(alpha=0.1),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
    'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42),
    'LightGBM': lgb.LGBMRegressor(n_estimators=100, random_state=42)
}

from sklearn.model_selection import train_test_split, cross_val_score

# Définition de la variable cible y
y = df['Automation_Probability_2030']

# Split classique
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")

# Time-based split (si données temporelles)
# Stratified split (si distribution inégale de la cible)

from sklearn.model_selection import KFold, cross_val_score

kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# Itérer sur chaque modèle et effectuer la validation croisée
for model_name, model_instance in models.items():
    print(f"\n--- Évaluation du modèle : {model_name} ---")
    cv_scores = cross_val_score(model_instance, X_train, y_train,
                                cv=kfold, scoring='r2', n_jobs=-1) # n_jobs=-1 pour paralléliser
    print(f"R² moyen CV: {cv_scores.mean():.3f} (±{cv_scores.std():.3f})")

"""# Task
Perform descriptive statistical analysis of the `df` DataFrame by displaying summary statistics for all numerical columns and generating histograms for 'Average_Salary', 'Years_Experience', 'AI_Exposure_Index', and 'Automation_Probability_2030'. Then, interpret these results to summarize the main characteristics of the data and identify potential next steps.

## Afficher les statistiques descriptives

### Subtask:
Afficher les statistiques descriptives (moyenne, écart-type, min, max, etc.) pour toutes les colonnes numériques du DataFrame `df`.

**Reasoning**:
The subtask requires displaying descriptive statistics for numerical columns. The `.describe()` method is perfect for this, as it provides mean, standard deviation, min, max, and quartile values.
"""

print("Statistiques descriptives pour les colonnes numériques :")
print(df.describe())

"""**Reasoning**:
The next part of the subtask is to generate histograms for the specified numerical columns to visualize their distributions. I will use `matplotlib.pyplot` and `seaborn` for this purpose, creating a figure with subplots for better organization.


"""

import matplotlib.pyplot as plt
import seaborn as sns

# List of columns for which to generate histograms
histogram_cols = ['Average_Salary', 'Years_Experience', 'AI_Exposure_Index', 'Automation_Probability_2030']

# Create a figure and a set of subplots
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 10))
axes = axes.flatten() # Flatten the 2x2 array of axes for easy iteration

# Generate histograms for each specified column
for i, col in enumerate(histogram_cols):
    sns.histplot(df[col], kde=True, ax=axes[i], color='skyblue')
    axes[i].set_title(f'Distribution of {col}', fontsize=14)
    axes[i].set_xlabel(col, fontsize=12)
    axes[i].set_ylabel('Frequency', fontsize=12)
    axes[i].grid(True, linestyle='--', alpha=0.6)

plt.tight_layout()
plt.show()
print("Histograms for 'Average_Salary', 'Years_Experience', 'AI_Exposure_Index', and 'Automation_Probability_2030' have been generated.")

"""### Interpretation of Descriptive Statistics and Histograms

**Descriptive Statistics Analysis:**

*   **Average_Salary**: Ranges from approximately \$30,000 to \$150,000, with a mean of around \$89,000. This indicates a wide range of salaries, suggesting diverse job roles or experience levels within the dataset.
*   **Years_Experience**: Ranges from 0 to 29 years, with a mean of about 14.7 years. The standard deviation is 8.7 years, showing a significant spread in experience levels. The distribution appears relatively uniform across the range.
*   **AI_Exposure_Index**: This index ranges from 0 to 1, with a mean of 0.5. The standard deviation is 0.28, indicating a fairly even distribution of AI exposure across the dataset, rather than being heavily skewed to high or low exposure.
*   **Tech_Growth_Factor**: Ranges from 0.5 to 1.5, with a mean of about 0.99. This variable also shows a wide spread, suggesting varying levels of technological growth impact on different jobs.
*   **Automation_Probability_2030**: The target variable ranges from 0.05 to 0.95, with a mean of 0.50 and a standard deviation of 0.24. This indicates a broad spectrum of automation probabilities, which is good for modeling as it shows variability in the target.
*   **Skill_1 to Skill_10**: These skill scores are all between 0 and 1, with means around 0.5 and similar standard deviations. This implies that the skills are normalized and generally well-distributed, likely representing different facets of job requirements.
*   **Derived Features (Salary_Experience_Ratio, Total_Skills_Score, AI_Tech_Interaction)**: These newly created features show their own distributions and ranges, which can be valuable for capturing more complex relationships in the data.

**Histogram Analysis:**

*   **Average_Salary**: The histogram shows a relatively uniform distribution across the salary range, with no strong peaks or skew, indicating a diverse salary structure within the dataset.
*   **Years_Experience**: The distribution of years of experience is also fairly uniform, with no clear preference for junior, mid, or senior levels, suggesting a balanced representation of experience.
*   **AI_Exposure_Index**: The `AI_Exposure_Index` appears somewhat uniform, or perhaps slightly bimodal, indicating jobs with both lower and higher AI exposure are present. This provides good variance for analysis.
*   **Automation_Probability_2030**: The histogram for the target variable `Automation_Probability_2030` also shows a relatively uniform distribution. This is ideal for predictive modeling as it indicates a good spread of target values across the entire range, making it easier for models to learn and distinguish different levels of automation risk.

**Overall Summary and Next Steps:**

The descriptive statistics and histograms reveal a dataset with well-distributed numerical features, particularly for `Average_Salary`, `Years_Experience`, `AI_Exposure_Index`, and `Automation_Probability_2030`. There are no immediate signs of extreme outliers or heavily skewed distributions that would necessitate complex transformations at this stage. The diversity in these key variables suggests that the dataset is suitable for training a model to predict `Automation_Probability_2030`.

**Potential Next Steps:**

1.  **Feature Engineering Review**: While some features were engineered, further exploration of interactions or polynomial features might be beneficial.
2.  **Multicollinearity Check**: The VIF analysis (from a previous cell) showed high multicollinearity for some features (`AI_Exposure_Index`, `Tech_Growth_Factor`, `Total_Skills_Score`, `AI_Tech_Interaction`). Addressing this, possibly by removing highly correlated features or using PCA, would be a critical next step before final model training.
3.  **Model Selection and Hyperparameter Tuning**: Given the baseline model performance (R-squared of 1.000 for several models, which is suspicious and likely indicates data leakage or overfitting during cross-validation due to the target being present in X, specifically `Automation_Probability_2030` within X), careful model selection and robust hyperparameter tuning for the chosen models will be crucial. The `Automation_Probability_2030` column should be removed from `X` to correctly evaluate the models.
4.  **Error Analysis**: For the `Lasso Regression` model which showed a poor R-squared, further investigation into why it performed so poorly compared to other models is warranted.
5.  **Re-evaluate Data Split**: Ensure the data split for training and testing is appropriate, especially if temporal dependencies exist (though not explicitly stated).
6.  **Outlier Detection and Handling**: Although not apparent from histograms, a more formal outlier detection method could be applied to refine the data if necessary.

## Summary:

### Data Analysis Key Findings

*   **Average Salary** ranges from approximately \$30,000 to \$150,000, with a mean of around \$89,000, indicating a wide and diverse salary structure across the dataset. The histogram shows a relatively uniform distribution.
*   **Years of Experience** varies from 0 to 29 years, with a mean of about 14.7 years and a standard deviation of 8.7 years, suggesting a broad spread of experience levels. Its distribution is also fairly uniform.
*   **AI Exposure Index** ranges from 0 to 1, with a mean of 0.5, indicating a fairly even distribution of AI exposure across the dataset, rather than being heavily skewed. The histogram appears somewhat uniform or slightly bimodal.
*   **Automation Probability 2030** (the target variable) spans from 0.05 to 0.95, with a mean of 0.50. This wide and relatively uniform distribution is favorable for predictive modeling as it provides sufficient variability.
*   All numerical features, including various skill scores and derived features, are generally well-distributed, with no immediate signs of extreme outliers or heavily skewed distributions, making the dataset suitable for further analysis and modeling.

### Insights or Next Steps

*   **Address Multicollinearity**: Prioritize checking and handling multicollinearity, especially for features previously identified (e.g., `AI_Exposure_Index`, `Tech_Growth_Factor`, `Total_Skills_Score`, `AI_Tech_Interaction`), before final model training.
*   **Refine Model Training**: Carefully re-evaluate model selection and hyperparameter tuning, ensuring the target variable (`Automation_Probability_2030`) is correctly excluded from the feature set (`X`) during training to avoid data leakage, as suggested by suspicious R-squared values in previous (unshown) model evaluations.
"""

import pandas as pd

# Créer un DataFrame temporaire pour l'analyse des résidus par catégorie
# Nous utilisons X_test car c'est sur cet ensemble que y_pred a été calculé
analysis_df = pd.DataFrame({
    'Actual': y_test,
    'Predicted': y_pred,
    'Residual': y_test - y_pred,
    'Risk_Category': X_test['Risk_Category']
})

# Regrouper par Risk_Category et calculer les statistiques descriptives
risk_category_analysis = analysis_df.groupby('Risk_Category').agg(
    mean_actual=('Actual', 'mean'),
    mean_predicted=('Predicted', 'mean'),
    mean_residual=('Residual', 'mean'),
    std_residual=('Residual', 'std'),
    count=('Actual', 'count')
).reset_index()

# Afficher le tableau d'analyse
print("\nAnalyse Matricielle des Prédictions par Catégorie de Risque :")
display(risk_category_analysis)
print("\nCette analyse montre les performances du modèle (moyennes réelles, prédites, et résiduelles) pour chaque catégorie de risque.")

"""### Interprétation de l'Analyse Matricielle des Prédictions

Ce tableau d'analyse matricielle segmentée par `'Risk_Category'` offre une vue détaillée de la performance de votre modèle à travers les différentes catégories de risque d'automatisation.

**Observations Clés :**

*   **`mean_actual` vs. `mean_predicted`** : En comparant ces deux colonnes, nous pouvons évaluer si le modèle prédit avec précision la moyenne de la probabilité d'automatisation pour chaque catégorie de risque. Idéalement, ces valeurs devraient être très proches.
*   **`mean_residual`** : Une `mean_residual` proche de zéro pour une catégorie indique que le modèle n'a pas de biais systématique (il ne sur-estime ni ne sous-estime constamment) pour cette catégorie. Des valeurs positives ou négatives significatives pourraient suggérer un biais du modèle pour cette catégorie spécifique.
*   **`std_residual`** : L'écart-type des résidus (`std_residual`) mesure la variabilité des erreurs de prédiction au sein de chaque catégorie. Une faible valeur indique des prédictions plus cohérentes pour cette catégorie, tandis qu'une valeur élevée suggère une plus grande dispersion des erreurs, ce qui signifie que le modèle est moins fiable pour ces cas.
*   **`count`** : Le nombre d'échantillons (`count`) dans chaque catégorie est important pour évaluer la robustesse des statistiques calculées. Les catégories avec un grand nombre d'échantillons fournissent des statistiques plus fiables.

**Implications :**

*   **Performance Segmentée** : Cette analyse permet de détecter si le modèle est plus performant sur certaines catégories de risque que sur d'autres. Par exemple, le modèle pourrait être très précis pour les catégories de "faible risque" mais avoir plus de difficultés avec les catégories de "haut risque".
*   **Identification de Biais** : Des biais systématiques (résidus moyens non nuls) dans certaines catégories pourraient indiquer que le modèle n'a pas appris toutes les nuances spécifiques à ces groupes, ou que des caractéristiques supplémentaires sont nécessaires pour les prédire plus précisément.
*   **Amélioration Ciblée** : Si des problèmes de performance sont identifiés dans des catégories spécifiques, cela peut orienter les efforts d'amélioration du modèle, par exemple en collectant plus de données pour ces catégories, en utilisant des techniques de sur-échantillonnage/sous-échantillonnage, ou en ajoutant des caractéristiques spécifiques qui différencient mieux ces groupes.
"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Get feature importances from the best-tuned model
feature_importances = best_tuned_model.feature_importances_

# Get feature names from X_features (defined earlier)
feature_names = X_features

# Create a DataFrame for better visualization
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': feature_importances
})

# Sort features by importance in descending order
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Plot feature importances
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=importance_df, palette='viridis')
plt.title('Feature Importance for Gradient Boosting Model', fontsize=16)
plt.xlabel('Importance', fontsize=12)
plt.ylabel('Feature', fontsize=12)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

print("Feature importance plot generated successfully.")

"""### Interprétation de l'Importance des Caractéristiques

Ce graphique montre l'importance relative de chaque caractéristique utilisée par le modèle Gradient Boosting pour prédire la probabilité d'automatisation des emplois.

**Observations Clés :**

*   **Identifier les facteurs déterminants** : Les barres les plus longues indiquent les caractéristiques qui ont le plus grand impact sur les prédictions du modèle. Cela permet de comprendre quels aspects d'un emploi sont les plus influents dans la détermination de son risque d'automatisation.
*   **Hiérarchie des caractéristiques** : La visualisation ordonnée des caractéristiques permet de voir clairement la hiérarchie de leur contribution. Par exemple, si l'`AI_Exposure_Index` ou le `Total_Skills_Score` apparaissent en tête, cela suggère qu'ils sont des indicateurs clés.
*   **Informations pour la prise de décision** : Ces informations sont cruciales pour les décideurs. Si certaines compétences ou niveaux d'expérience sont moins importants, cela pourrait influencer les programmes de formation ou les stratégies de développement des talents.

**Implications :**

*   **Focus sur les variables clés** : Les efforts futurs d'analyse ou de collecte de données pourraient se concentrer sur les caractéristiques les plus importantes pour affiner encore le modèle.
*   **Soutien à la stratégie** : Ces insights peuvent aider à formuler des stratégies pour atténuer l'impact de l'automatisation en se concentrant sur le développement des compétences ou des rôles qui sont moins à risque.

Cette analyse fournit une vision claire des moteurs sous-jacents de la probabilité d'automatisation selon notre modèle, complétant ainsi l'évaluation de sa performance prédictive.
"""

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Calculate residuals (errors)
# y_test and y_pred are already available from the model evaluation step
residuals = y_test - y_pred

# Create a figure with two subplots
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Plot 1: Residuals vs. Predicted Values
sns.scatterplot(x=y_pred, y=residuals, ax=axes[0], alpha=0.6)
axes[0].axhline(y=0, color='r', linestyle='--', linewidth=2)
axes[0].set_title('Residuals vs. Predicted Automation Probability', fontsize=14)
axes[0].set_xlabel('Predicted Automation Probability', fontsize=12)
axes[0].set_ylabel('Residuals (Actual - Predicted)', fontsize=12)
axes[0].grid(True, linestyle='--', alpha=0.6)

# Plot 2: Histogram of Residuals
sns.histplot(residuals, kde=True, ax=axes[1], color='skyblue')
axes[1].set_title('Distribution of Residuals', fontsize=14)
axes[1].set_xlabel('Residuals', fontsize=12)
axes[1].set_ylabel('Frequency', fontsize=12)
axes[1].grid(True, linestyle='--', alpha=0.6)

plt.tight_layout()
plt.show()
print("Residual plots generated successfully.")

"""### Interprétation de l'Analyse des Résidus

L'analyse des résidus est cruciale pour évaluer la validité des hypothèses d'un modèle de régression et pour identifier les domaines où le modèle pourrait être amélioré.

**1. Nuage de points des Résidus par rapport aux Valeurs Prédites :**

*   **Idéalement** : Les points devraient être dispersés de manière aléatoire autour de la ligne horizontale `y=0`, sans aucun motif discernable (aucune forme en U, en entonnoir, etc.). Cela indique que les erreurs du modèle sont aléatoires et indépendantes des prédictions.
*   **Observation actuelle** : Si le nuage de points montre une dispersion aléatoire et uniforme des résidus autour de zéro, cela suggère que le modèle est bien ajusté et que les hypothèses de linéarité et d'homoscédasticité (variance constante des erreurs) sont raisonnablement respectées. Tout motif visible (par exemple, des résidus qui s'élargissent ou se rétrécissent à mesure que les valeurs prédites augmentent) pourrait indiquer un problème comme l'hétéroscédasticité ou une spécification de modèle incorrecte (par exemple, une relation non linéaire qui n'est pas capturée).

**2. Histogramme de la Distribution des Résidus :**

*   **Idéalement** : La distribution des résidus devrait être approximativement normale (en forme de cloche) et centrée autour de zéro. Cela est une hypothèse fondamentale de nombreux modèles de régression (notamment la régression linéaire).
*   **Observation actuelle** : Si l'histogramme des résidus est centré autour de zéro et présente une forme qui ressemble à une distribution normale, cela renforce la confiance dans la validité statistique du modèle. Une distribution asymétrique, bimodale ou un pic prononcé pourrait indiquer des problèmes, tels que la présence d'outliers, un biais dans le modèle, ou que le modèle ne capture pas toutes les informations pertinentes des données.

**Conclusion de l'analyse des résidus :**

Cette analyse fournit des informations détaillées sur la performance de votre `Gradient Boosting` model. Un modèle robuste aura des résidus qui sont petits, non corrélés avec les valeurs prédites et distribués normalement. Si des motifs apparaissent, cela pourrait être une indication pour explorer d'autres caractéristiques, transformer des variables ou considérer des modèles plus complexes.
"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 8))
sns.scatterplot(x='Automation_Probability_2030', y='Predicted_Automation_Probability_2030', data=df, alpha=0.6)
plt.plot([df['Automation_Probability_2030'].min(), df['Automation_Probability_2030'].max()],
         [df['Automation_Probability_2030'].min(), df['Automation_Probability_2030'].max()],
         color='red', linestyle='--', lw=2, label='Perfect Prediction Line')
plt.title('Actual vs. Predicted Automation Probability', fontsize=16)
plt.xlabel('Actual Automation Probability', fontsize=12)
plt.ylabel('Predicted Automation Probability', fontsize=12)
plt.legend()
plt.grid(True, linestyle='--', alpha=0.7)
plt.show()

"""### Interprétation du Graphique "Actual vs. Predicted Automation Probability"

Ce nuage de points visualise directement la performance de votre modèle en comparant les probabilités d'automatisation réelles (`Automation_Probability_2030`) avec les probabilités prédites (`Predicted_Automation_Probability_2030`).

**Observations Clés :**

*   **Ligne de Prédiction Parfaite (rouge en pointillés)** : Cette ligne représente le scénario idéal où chaque prédiction serait exactement égale à la valeur réelle. Plus les points du nuage sont proches de cette ligne, plus le modèle est précis.
*   **Dispersion des Points** : La majorité des points sont regroupés autour de la ligne de prédiction parfaite, ce qui indique que le modèle a une bonne capacité à prédire la probabilité d'automatisation. La concentration des points le long de cette ligne est cohérente avec le score R² de 0.865 obtenu sur l'ensemble de test, confirmant une forte corrélation entre les valeurs réelles et prédites.
*   **Écarts** : Bien que la plupart des points soient proches de la ligne, il peut y avoir quelques points plus éloignés. Ceux-ci représentent les cas où le modèle a fait des prédictions moins précises. Analyser ces points (erreurs résiduelles) pourrait aider à identifier des cas particuliers où le modèle pourrait être amélioré, ou des caractéristiques non capturées.

**Conclusion :**

Le graphique confirme visuellement que le modèle Gradient Boosting affiné est performant pour prédire la probabilité d'automatisation des emplois. La forte concentration des points autour de la ligne de prédiction parfaite démontre sa fiabilité et sa capacité à généraliser sur l'ensemble des données.

### Interprétation des Prédictions

Nous avons ajouté une nouvelle colonne, `Predicted_Automation_Probability_2030`, à votre DataFrame `df`. Cette colonne contient la probabilité d'automatisation prédite par notre modèle Gradient Boosting affiné pour chaque poste.

**Observations Clés :**

*   En comparant les colonnes `Automation_Probability_2030` (valeurs réelles) et `Predicted_Automation_Probability_2030` (prédictions), nous pouvons observer que les prédictions du modèle sont globalement proches des valeurs réelles. Les 5 premières lignes affichées donnent un aperçu de cette correspondance. Pour une évaluation plus complète, nous avons déjà calculé un R² de 0.865 sur l'ensemble de test, ce qui indique que le modèle explique environ 86.5% de la variance des probabilités d'automatisation.
*   La présence de ces prédictions directement dans le DataFrame `df` permet une analyse plus fine au niveau de chaque poste. Vous pouvez désormais trier le DataFrame par probabilité prédite, identifier les postes les plus à risque ou les moins à risque d'automatisation selon le modèle, et les analyser en fonction de leurs autres caractéristiques (salaire, expérience, exposition à l'IA, compétences, etc.).

**Implications :**

Ces prédictions peuvent être utilisées pour :

1.  **Planification Stratégique** : Aider les entreprises ou les décideurs politiques à anticiper les changements dans le marché du travail et à planifier des stratégies de reconversion ou de formation pour les postes à haut risque.
2.  **Orientation Professionnelle** : Fournir des informations aux individus sur le risque d'automatisation de leur carrière actuelle ou future, les aidant à choisir des compétences à développer.
3.  **Analyse d'Impact** : Étudier quels facteurs (compétences, salaire, expérience, etc.) sont le plus fortement liés à une haute ou basse probabilité d'automatisation selon le modèle.

Cette étape finalise l'application de votre modèle prédictif sur l'ensemble de vos données, vous donnant des insights concrets sur la probabilité d'automatisation des emplois.
"""

# Générer les prédictions pour l'ensemble complet des données (en utilisant X)
# Assurez-vous que X est dans le bon ordre de colonnes comme utilisé pour l'entraînement
# Ici, X a été défini avec des colonnes spécifiques dans une cellule précédente.
# Si X n'est pas le jeu de données complet mais seulement X_train/X_test, il faut reconstruire X pour la prédiction complète.

# Nous allons recréer X pour la prédiction sur le dataframe complet df
X_for_prediction = df[X_features] # X_features a été défini précédemment

df['Predicted_Automation_Probability_2030'] = best_tuned_model.predict(X_for_prediction)

print("Prédictions ajoutées au DataFrame 'df'. Voici les 5 premières lignes avec la nouvelle colonne :")
display(df[['Automation_Probability_2030', 'Predicted_Automation_Probability_2030']].head())

"""### Interprétation des Prédictions

Nous avons ajouté une nouvelle colonne, `Predicted_Automation_Probability_2030`, à votre DataFrame `df`. Cette colonne contient la probabilité d'automatisation prédite par notre modèle Gradient Boosting affiné pour chaque poste.

**Observations Clés :**

*   En comparant les colonnes `Automation_Probability_2030` (valeurs réelles) et `Predicted_Automation_Probability_2030` (prédictions), nous pouvons observer que les prédictions du modèle sont globalement proches des valeurs réelles. Les 5 premières lignes affichées donnent un aperçu de cette correspondance. Pour une évaluation plus complète, nous avons déjà calculé un R² de 0.865 sur l'ensemble de test, ce qui indique que le modèle explique environ 86.5% de la variance des probabilités d'automatisation.
*   La présence de ces prédictions directement dans le DataFrame `df` permet une analyse plus fine au niveau de chaque poste. Vous pouvez désormais trier le DataFrame par probabilité prédite, identifier les postes les plus à risque ou les moins à risque d'automatisation selon le modèle, et les analyser en fonction de leurs autres caractéristiques (salaire, expérience, exposition à l'IA, compétences, etc.).

**Implications :**

Ces prédictions peuvent être utilisées pour :

1.  **Planification Stratégique** : Aider les entreprises ou les décideurs politiques à anticiper les changements dans le marché du travail et à planifier des stratégies de reconversion ou de formation pour les postes à haut risque.
2.  **Orientation Professionnelle** : Fournir des informations aux individus sur le risque d'automatisation de leur carrière actuelle ou future, les aidant à choisir des compétences à développer.
3.  **Analyse d'Impact** : Étudier quels facteurs (compétences, salaire, expérience, etc.) sont le plus fortement liés à une haute ou basse probabilité d'automatisation selon le modèle.

Cette étape finalise l'application de votre modèle prédictif sur l'ensemble de vos données, vous donnant des insights concrets sur la probabilité d'automatisation des emplois.

# Task
Correct the data leakage by redefining the feature set `X` to exclude `Automation_Probability_2030`, then re-split the data into training and test sets. Subsequently, re-evaluate the predefined regression models using cross-validation on the corrected data, select the best model, perform hyperparameter tuning, and finally, evaluate the refined model's performance on the test set to predict the probability of job automation.

## Redéfinir les caractéristiques (X)

### Subtask:
S'assurer que la variable cible Automation_Probability_2030 est bien exclue de l'ensemble des caractéristiques X pour éviter la fuite de données (data leakage).

**Reasoning**:
The subtask requires redefining the `X` features by excluding the target variable `Automation_Probability_2030` and selecting all relevant numerical features, including the encoded categorical features, while avoiding redundant ones. I will explicitly define the list of columns to be included in `X` to ensure correctness.
"""

import pandas as pd

# Redefine X to exclude the target variable 'Automation_Probability_2030'
# and select all relevant numerical and encoded categorical features.
# The original 'Job_Title' and 'Job_Title_encoded' are excluded in favor of 'Job_Title_te'.
# 'Education_Level' and 'Risk_Category' are included as they were label-encoded in-place and are now numerical features.

X_features = [
    'Average_Salary',
    'Years_Experience',
    'AI_Exposure_Index',
    'Tech_Growth_Factor',
    'Skill_1', 'Skill_2', 'Skill_3', 'Skill_4', 'Skill_5',
    'Skill_6', 'Skill_7', 'Skill_8', 'Skill_9', 'Skill_10',
    'Education_Level',
    'Risk_Category',
    'Job_Title_te',
    'Salary_Experience_Ratio',
    'Total_Skills_Score',
    'AI_Tech_Interaction'
]

X = df[X_features]

print(f"New X shape: {X.shape}")
print("New X columns:\n", X.columns.tolist())

# Ensure the target variable 'Automation_Probability_2030' is defined correctly
y = df['Automation_Probability_2030']
print(f"y shape: {y.shape}")

"""## Re-séparer les données en ensembles d'entraînement et de test

### Subtask:
Re-split the data into training and test sets (X_train, X_test, y_train, y_test) using the newly defined feature set X and the target variable y.

**Reasoning**:
The subtask requires re-splitting the data into training and test sets using the previously defined X and y. The `train_test_split` function from `sklearn.model_selection` is the appropriate tool for this, along with printing the shapes for verification.
"""

from sklearn.model_selection import train_test_split

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")

"""## Ré-évaluer les modèles avec validation croisée

### Subtask:
Exécuter à nouveau la validation croisée sur l'ensemble des modèles définis dans le dictionnaire `models` en utilisant les ensembles `X_train` et `y_train` mis à jour, afin d'obtenir des scores R² fiables sans fuite de données.

**Reasoning**:
The subtask requires re-evaluating the models with cross-validation using the updated `X_train` and `y_train` sets. The code will iterate through the `models` dictionary, perform K-Fold cross-validation for each model, and print the mean and standard deviation of the R² scores, as specified in the instructions.
"""

from sklearn.model_selection import KFold, cross_val_score

kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# Itérer sur chaque modèle et effectuer la validation croisée
for model_name, model_instance in models.items():
    print(f"\n--- Évaluation du modèle : {model_name} ---")
    cv_scores = cross_val_score(model_instance, X_train, y_train,
                                cv=kfold, scoring='r2', n_jobs=-1) # n_jobs=-1 pour paralléliser
    print(f"R² moyen CV: {cv_scores.mean():.3f} (±{cv_scores.std():.3f})")

"""## Sélectionner le meilleur modèle

### Subtask:
Identifier le modèle le plus performant basé sur les résultats de la validation croisée, en tenant compte de la performance moyenne et de l'écart-type.

**Reasoning**:
To identify the best model, I will iterate through the cross-validation results, find the model with the highest average R-squared score, and store its name and instance for future steps.
"""

best_model_name = None
best_r2_score = -np.inf
best_model_instance = None

# Dictionnaire pour stocker les scores R² moyens pour chaque modèle
model_r2_scores = {}

kfold = KFold(n_splits=5, shuffle=True, random_state=42)

for model_name, model_instance in models.items():
    cv_scores = cross_val_score(model_instance, X_train, y_train,
                                cv=kfold, scoring='r2', n_jobs=-1)
    mean_r2 = cv_scores.mean()
    model_r2_scores[model_name] = mean_r2

    if mean_r2 > best_r2_score:
        best_r2_score = mean_r2
        best_model_name = model_name
        best_model_instance = model_instance

print(f"Le meilleur modèle est '{best_model_name}' avec un R² moyen CV de {best_r2_score:.3f}.")
print(f"Instance du meilleur modèle stockée dans 'best_model_instance': {best_model_instance}")

"""## Affiner les hyperparamètres (Tuning)

### Subtask:
Appliquer des techniques d'optimisation des hyperparamètres (par exemple, GridSearchCV ou RandomizedSearchCV) pour le modèle sélectionné afin d'améliorer ses performances.

**Reasoning**:
The subtask requires applying hyperparameter optimization techniques. I will start by importing the necessary `GridSearchCV` and `GradientBoostingRegressor` classes to define the parameter grid and set up the GridSearchCV object.
"""

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import GradientBoostingRegressor

# Define the parameter grid for GradientBoostingRegressor
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 4, 5]
}

# Instantiate GridSearchCV with the best_model_instance (which is GradientBoostingRegressor)
grid_search = GridSearchCV(estimator=best_model_instance,
                           param_grid=param_grid,
                           cv=kfold,
                           scoring='r2',
                           n_jobs=-1,
                           verbose=1) # verbose to see the progress

print("GridSearchCV object instantiated and parameter grid defined.")

"""**Reasoning**:
Now that the GridSearchCV object is instantiated and the parameter grid is defined, the next step is to fit it to the training data to find the best hyperparameters.


"""

print("Fitting GridSearchCV...")
grid_search.fit(X_train, y_train)

print("Optimal hyperparameters found:", grid_search.best_params_)
print("Best R² score with optimal hyperparameters:", grid_search.best_score_)

"""**Reasoning**:
The subtask is to evaluate the refined model's performance on the test set. I will first extract the best estimator from the GridSearchCV results and then evaluate its performance on the `X_test` and `y_test` data using appropriate metrics.


"""

from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Get the best estimator from GridSearchCV
best_tuned_model = grid_search.best_estimator_

print(f"Best tuned model: {best_tuned_model}")

# Make predictions on the test set
y_pred = best_tuned_model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f"\nPerformance of the best tuned model on the test set:")
print(f"Mean Squared Error (MSE): {mse:.3f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.3f}")
print(f"R-squared (R²): {r2:.3f}")

"""## Final Task

### Subtask:
Présenter les résultats de l'analyse prédictive, y compris le modèle choisi, ses performances et les implications pour la prédiction de la probabilité d'automatisation.

## Summary:

### Data Analysis Key Findings
*   **Data Leakage Addressed**: The feature set `X` was successfully redefined to exclude the target variable `Automation_Probability_2030`, now comprising 20 features, including `Average_Salary`, `Years_Experience`, `AI_Exposure_Index`, `Tech_Growth_Factor`, and encoded categorical features.
*   **Data Re-split**: The dataset was re-split into training (2400 samples) and testing (600 samples) sets, ensuring an 80/20 ratio for model development and unbiased evaluation.
*   **Model Performance Post-Correction**: After correcting for data leakage, ensemble models significantly outperformed linear models in cross-validation:
    *   Gradient Boosting: Mean R² CV of 0.845 (\$\pm\$0.011)
    *   Random Forest: Mean R² CV of 0.842 (\$\pm\$0.011)
    *   LightGBM: Mean R² CV of 0.835 (\$\pm\$0.012)
    *   XGBoost: Mean R² CV of 0.817 (\$\pm\$0.010)
    *   Linear/Ridge Regression: Mean R² CV of 0.135 (\$\pm\$0.027)
    *   Lasso Regression: Mean R² CV of -0.002 (\$\pm\$0.002)
*   **Best Model Selection**: `Gradient Boosting` was identified as the best model based on its superior average cross-validation R² score of 0.845.
*   **Hyperparameter Tuning Results**: Hyperparameter tuning using `GridSearchCV` for the Gradient Boosting model resulted in optimal parameters: `learning_rate=0.05`, `max_depth=3`, and `n_estimators=100`, yielding a cross-validation R² score of approximately 0.848.
*   **Final Model Performance**: The best-tuned Gradient Boosting model achieved strong performance on the unseen test set with an R² of 0.865, a Mean Squared Error (MSE) of 0.009, and a Root Mean Squared Error (RMSE) of 0.093, indicating accurate predictions of automation probability.

### Insights or Next Steps
*   The Gradient Boosting model, after careful feature engineering and hyperparameter tuning, provides a robust and reliable prediction of job automation probability, demonstrating strong generalization capabilities on unseen data.
*   To further enhance the model's interpretability and provide actionable insights, an analysis of feature importance for the selected Gradient Boosting model could be conducted to identify which job characteristics are the strongest drivers of automation probability.
"""